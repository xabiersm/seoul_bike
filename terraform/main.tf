terraform {
  required_providers {
    google = {
        source = "hashicorp/google"
        version = "~> 5.0"
    }
  }
}

provider "google" {
  credentials = file(var.credentials)
  project     = var.project
  region      = var.region
}

#create a bucket to storage the data from the csv
resource "google_storage_bucket" "data-bucket" {
  name          = var.gcs_data_bucket_name
  location = var.location
  force_destroy = true


  lifecycle_rule {
    condition {
      age = 1
    }
    action {
      type = "AbortIncompleteMultipartUpload"
    }
  }
}

#create a bucket for the Dataproc cluster
resource "google_storage_bucket" "cluster-bucket" {
  name = var.gcs_cluster_bucket_name
  location = var.location

  storage_class = var.gcs_storage_class
  uniform_bucket_level_access = true

  versioning {
    enabled = true
  }

  lifecycle_rule {
    action {
      type = "Delete"
    }
    condition {
      age = 30  // days
    }
  }

  force_destroy = true
}

#data warehouse to store data
resource "google_bigquery_dataset" "demo_dataset" {
  dataset_id = var.bq_dataset_name
  location = var.location
}

#dataproc cluster to store the data generated by the spark jobs
resource "google_dataproc_cluster" "mycluster" {
  name     = var.dataproc_cluster_name
  region   = var.region
  graceful_decommission_timeout = "120s"

  cluster_config {
    staging_bucket = var.gcs_cluster_bucket_name

    master_config {
      num_instances = 1
      machine_type  = "e2-medium"
      disk_config {
        boot_disk_type    = "pd-ssd"
        boot_disk_size_gb = 30
      }
    }

    worker_config {
      num_instances    = 2
      machine_type     = "e2-medium"
    }

    preemptible_worker_config {
      num_instances = 0
    }

    # Override or set some custom properties
    software_config {
      image_version = "2.0.35-debian10"
      override_properties = {
        "dataproc:dataproc.allow.zero.workers" = "true"
      }
    }

    gce_cluster_config {
      tags = ["seoul-bike-trips"]
      # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.
      service_account = var.gce_service_account
      service_account_scopes = [
        "cloud-platform"
      ]
      zone = var.dataproc_cluster_zone
    }

    # You can define multiple initialization_action blocks
    initialization_action {
      script      = "gs://dataproc-initialization-actions/stackdriver/stackdriver.sh"
      timeout_sec = 500
    }
  }
}